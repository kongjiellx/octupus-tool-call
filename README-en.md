# octupus-tool-call 

[English|[中文](https://github.com/kongjiellx/octupus-tool-call/blob/main/README.md)]
## Project Overview  
This is a project focused on training a tool call (function calling) model. It will open-source all data, code, models, and inference schemes involved in the model training process.  

## Project Goal & Evaluation Plan  
Currently, [BFCL](https://gorilla.cs.berkeley.edu/leaderboard.html) is used as the evaluation benchmark. The goal of this project is to improve the model's score on BFCL as much as possible. We will continue iterating and synchronize each version’s improvements here.  

## Function Calling  
Function calling is a method that allows large language models (LLMs) to output function call instructions. As LLMs are language models, during training, they are exposed to various text formats and tend to respond to users in natural language. Generally, they have weak abilities to output formatted function call instructions.  

This project mainly optimizes the issue from two aspects:  
1. Training the LLM using function calling data in a specific format.  
2. Enforcing constraints on the format during inference.  

## Key Points of the Overall Plan  
1. Using the pre-trained model from the qwen2.5 series, fine-tuning it on open-source dialogue data and function calling data. We avoid using instruct models because we need to change the chat template. To facilitate inference and enforce constraints, we use some special tokens.  
2. Implementing output formatting via [lm-format-enforcer](https://github.com/noamgat/lm-format-enforcer), ensuring that the function call instructions generated by the model are valid.  
3. Compatible with the OpenAI API server, supporting tool_choice, parallel tool calls, and ensuring output legality.  

## Directory Overview  
- `utils/edit_tokenizer_and_model.py`: Edit the tokenizer, add special tokens, and initialize related tokens.  
- `versions/v1`: The training scripts for the first version of the model (future updates will include v2, v3, etc.)  
    - `train_stg1.sh`:  
        - Re-trains qwen2.5-72b-instruct on openhermes2.5 with 1M data.  
        - Wild chat.  
    - `train_stg2.sh`:  
        - Continues training on the checkpoint from stg1.  
        - Mixes a small amount of stg1 data.  
        - Uses NousResearch/hermes-function-calling-v1 data.  
        - Uses hqfx/fc_zh_hard (longer Chinese FC data synthesized using GPT-4).  
    - Stg1 training uses 32 GPUs, while stg2 uses 8 GPUs. Other parameters are specified in the training scripts.  
- `inference`:  
    - OpenAI-compatible model service, using the structured outputs technique.  
    - Usage:  
      ```  
      python oai_server.py --model /your/model --tensor-parallel-size 1 --max-model-len 8192  
      ```

## Evaluation Method  
1. Start the model service using `oai_server.py`.  
2. No changes are necessary; directly use BFCL for testing. The service itself does not need the model name, but to invoke the logic for OpenAI FC models in BFCL code, fill in a supported GPT FC model name, such as:  
    ```  
    OPENAI_BASE_URL=http://localhost:8000 bfcl generate --model gpt-4-turbo-2024-04-09-FC --num-threads 8 --test-category all  
    ```

## Iteration Record  
### V1  

**Model and Data:** [Hugging Face Collection](https://huggingface.co/collections/hqfx/hqfx-octupus-tool-call-v1-6752bc1b3d5dc4e06f394e59)  

**Evaluation Results:**  
| Overall Acc | Model                        | Non-Live AST Acc | Non-Live Simple AST | Non-Live Multiple AST | Non-Live Parallel AST | Non-Live Parallel Multiple AST | Non-Live Exec Acc | Non-Live Simple Exec | Non-Live Multiple Exec | Non-Live Parallel Exec | Non-Live Parallel Multiple Exec | Live Acc | Live Simple AST | Live Multiple AST | Live Parallel AST | Live Parallel Multiple AST | Multi Turn Acc | Multi Turn Base | Multi Turn Miss Func | Multi Turn Miss Param | Multi Turn Long Context | Relevance Detection | Irrelevance Detection |  
|-------------|-----------------------------|------------------|---------------------|-----------------------|-----------------------|-------------------------------|------------------|---------------------|-----------------------|-----------------------|-------------------------------|---------|----------------|------------------|------------------|--------------------------|---------------|----------------|--------------------|--------------------|-----------------------|------------------|------------------|  
| 52.21%      | hqfx/octupus-tool-call-v1 | 80.69%           | 65.25%              | 94.00%               | 82.50%               | 81.00%                         | 83.70%           | 88.29%              | 92.00%               | 82.00%               | 72.50%                         | 73.92%  | 66.28%         | 69.72%           | 31.25%           | 33.33%                  | 0.12%         | 0.00%          | 0.00%              | 0.50%              | 0.00%                | 73.17%           | 84.46%           |

On the [BFCL leaderboard](https://gorilla.cs.berkeley.edu/leaderboard.html), it ranks 30th, slightly outperforming GPT-3.5. The multi-turn performance scored 0, and further analysis is needed. Future optimizations will focus on improving this area.